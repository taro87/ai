{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e429c4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:90% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:16pt;}\n",
       "div.text_cell_render.rendered_html{font-size:16pt;}\n",
       "div.output {font-size:12pt; \n",
       "las; font-size:16pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:16pt;padding:5px;}\n",
       "table.dataframe{font-size:16px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:90% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:16pt;}\n",
    "div.text_cell_render.rendered_html{font-size:16pt;}\n",
    "div.output {font-size:12pt; \n",
    "las; font-size:16pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:16pt;padding:5px;}\n",
    "table.dataframe{font-size:16px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773afc7",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">ch2. Ollama LLMí™œìš©ì˜ ê¸°ë³¸ ê°œë…</span>\n",
    "# 1. LLMì„ í™œìš©í•˜ì—¬ ë‹µë³€ ìƒì„±í•˜ê¸°\n",
    "## 1) Ollamaë¥¼ ì´ìš©í•œ ë¡œì»¬ LLM ì´ìš©\n",
    "- ì„±ëŠ¥ì€ GPT, Claude ê°™ì€ ëª¨ë¸ë³´ë‹¤ ë–¨ì–´ì§€ë‚˜, ê°œë…ì„¤ëª…ì„ ìœ„í•´ open source ëª¨ë¸ ì‚¬ìš©\n",
    "\n",
    "### ollama.com ë‹¤ìš´ë¡œë“œ -> ì„¤ì¹˜ -> ëª¨ë¸ pull\n",
    "- cmdì°½ì´ë‚˜ powershellì—ì„œ ollama run deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "773585ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today? ğŸ˜Š', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-12-09T02:15:31.4061129Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1986945700, 'load_duration': 1368706500, 'prompt_eval_count': 4, 'prompt_eval_duration': 76174100, 'eval_count': 16, 'eval_duration': 523079900, 'logprobs': None, 'model_name': 'deepseek-r1:1.5b', 'model_provider': 'ollama'}, id='lc_run--019b00e4-6f89-7f80-88e4-dd248a684336-0', usage_metadata={'input_tokens': 4, 'output_tokens': 16, 'total_tokens': 20})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "result = llm.invoke(\"hi\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3565f9",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ pull\n",
    "- cmdì°½ì´ë‚˜ powershellì°½ì—ì„œ ollama run llama3.2:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a271e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "result = llm.invoke(\"í•œêµ­ì˜ ìˆ˜ë„ê°€ ì–´ë””ì¸ì§€ ì•Œë ¤ì¤˜\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17014c71",
   "metadata": {},
   "source": [
    "## 2. openai í™œìš©\n",
    "- pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb669292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í™˜ê²½ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "742aba8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-5-nano\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\load\\serializable.py:116\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:978\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_async_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mAsyncClient(\n\u001b[0;32m    969\u001b[0m             proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy, verify\u001b[38;5;241m=\u001b[39mglobal_ssl_context\n\u001b[0;32m    970\u001b[0m         )\n\u001b[0;32m    971\u001b[0m     async_specific \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    972\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_async_client\n\u001b[0;32m    973\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _get_default_async_httpx_client(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: async_api_key_value,\n\u001b[0;32m    977\u001b[0m     }\n\u001b[1;32m--> 978\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_async_client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mAsyncOpenAI(\n\u001b[0;32m    979\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params,\n\u001b[0;32m    980\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39masync_specific,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    981\u001b[0m     )\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_async_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\openai\\_client.py:488\u001b[0m, in \u001b[0;36mAsyncOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    486\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    489\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m     )\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\",\n",
    "#                 api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "                )\n",
    "result = llm.invoke(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAI\n",
    "llm = AzureOpenAI(model='gpt-5-nano')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727c755",
   "metadata": {},
   "source": [
    "# 2. ë ì²´ì¸ ìŠ¤íƒ€ì¼ë¡œ í”„ë¡¬í”„íŠ¸ ì‘ì„±\n",
    "í”„ë¡¬í”„íŠ¸ : llmí˜¸ì¶œì‹œ ì“°ëŠ” ì§ˆë¬¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65b1bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "# llm.invoke(0)\n",
    "# PromptValue, str, BaseMessagesë¦¬ìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb57a0",
   "metadata": {},
   "source": [
    "## 1) ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©\n",
    "- PromptTemplateì„ ì‚¬ìš©í•˜ì—¬ ë³€ìˆ˜ê°€ í¬í•¨ëœ í…œí”Œë¦¿ì„ ì‘ì„±í•˜ë©´ PromptValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c666bb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='What is the capital of Korea?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The capital of South Korea is Seoul. However, it's worth noting that North Korea also claims the city of Pyongyang as its capital.\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-12-09T06:41:50.3172096Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4304503800, 'load_duration': 2884971000, 'prompt_eval_count': 32, 'prompt_eval_duration': 294747000, 'eval_count': 28, 'eval_duration': 1090196900, 'logprobs': None, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--019b01d8-381c-73e2-88a5-c150ca8c94cf-0', usage_metadata={'input_tokens': 32, 'output_tokens': 28, 'total_tokens': 60})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"What is the capital of {country}?\", # {}ì•ˆì˜ ê°’ì„ ìƒˆë¡œìš´ ê°’ìœ¼ë¡œ ëŒ€ì… ê°€ëŠ¥\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "# country = input('ì–´ëŠ ë‚˜ë¼ì˜ ìˆ˜ë„ë¥¼ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?')\n",
    "prompt = prompt_template.invoke({\"country\":\"Korea\"})\n",
    "print(prompt)\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8efe96",
   "metadata": {},
   "source": [
    "## 2) ë©”ì„¸ì§€ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ì‘ì„±\n",
    "- BaseMessage ë¦¬ìŠ¤íŠ¸\n",
    "- BaseMessage ìƒì†ë°›ì€ í´ë˜ìŠ¤ : AIMessage, HumanMessage, SystemMessage, ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c89f610d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"If you're referring to a country that isn't Italy or France, I'd be happy to help. What's the name of the country?\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-12-09T06:41:57.5197959Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1915790500, 'load_duration': 113341000, 'prompt_eval_count': 87, 'prompt_eval_duration': 579978200, 'eval_count': 30, 'eval_duration': 1187964300, 'logprobs': None, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--019b01d8-5d92-74a0-b561-5fe056251562-0', usage_metadata={'input_tokens': 87, 'output_tokens': 30, 'total_tokens': 117})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "message_list = [\n",
    "    SystemMessage(content=\"You are a helpful assistant!\"), # í˜ë¥´ì†Œë‚˜ ë¶€ì—¬\n",
    "    HumanMessage(content=\"What is the capital of Italy?\"), # ëª¨ë²”ì§ˆë¬¸\n",
    "    AIMessage(content=\"The capital of Italy is Rome.\"), # ëª¨ë²”ë‹µì•ˆ\n",
    "    HumanMessage(content=\"What is the capital of France?\"), # ëª¨ë²”ì§ˆë¬¸\n",
    "    AIMessage(content=\"The capital of Italy is Paris.\"), # ëª¨ë²”ë‹µì•ˆ\n",
    "    HumanMessage(content=\"What is the capital of {country}?\")\n",
    "]\n",
    "llm.invoke(message_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e90fc5",
   "metadata": {},
   "source": [
    "## 3) ChatPrompttemplate ì‚¬ìš©\n",
    "- BaseMessageë¦¬ìŠ¤íŠ¸ -> íŠœí”Œë¦¬ìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99a4d0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì–´ëŠ ë‚˜ë¼ ìˆ˜ë„ê°€ ê¶ê¸ˆí•˜ì„¸ìš”?korea\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The capital of South Korea is Seoul, and the capital of North Korea is Pyongyang. However, if you're referring to the official name of the country with Pyongyang as its northernmost city, that would be Democratic People's Republic of Korea (DPRK) or North Korea. If you're asking about the de facto capital, it's usually Beijing.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PromptTemplate : í”„ë¡¬í”„íŠ¸ì— ë³€ìˆ˜í¬í•¨\n",
    "# ChatPromptTemplate : SystemPromptì„¤ì •(í˜ë¥´ì†Œë‚˜), few shotì„¤ì •, ë³€ìˆ˜í¬í•¨\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "chatPrompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant!\"),\n",
    "    ('human',\"What is the capital of Italy?\"),\n",
    "    (\"ai\",\"The capital of Italy is Rome.\"),\n",
    "    ('human',\"What is the capital of France?\"),\n",
    "    (\"ai\",\"The capital of Italy is Paris.\"),\n",
    "    ('human',\"What is the capital of {country}?\")\n",
    "])\n",
    "country = input(\"ì–´ëŠ ë‚˜ë¼ ìˆ˜ë„ê°€ ê¶ê¸ˆí•˜ì„¸ìš”?\")\n",
    "prompt = chatPrompt_template.invoke({\"country\":country})\n",
    "# print(\"í”„ë¡¬í”„íŠ¸ :\", prompt, type(prompt))\n",
    "result = llm.invoke(prompt)\n",
    "result.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b905b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì–´ëŠ ë‚˜ë¼ ìˆ˜ë„ê°€ ê¶ê¸ˆí•˜ì„¸ìš”?korea\n",
      "The capital of South Korea is Seoul, and the capital of North Korea is Pyongyang. However, if you're referring to the official administrative division that includes both countries, it would be Kaesong in North Korea or Incheon in South Korea.\n"
     ]
    }
   ],
   "source": [
    "chatPromptTemplate = ChatPromptTemplate.from_messages([\n",
    "    ('system','ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì •ë³´ ì „ë¬¸ ë„ìš°ë¯¸ì•¼'),\n",
    "    ('human','{country}ì˜ ìˆ˜ë„ê°€ ì–´ë””ì—ìš”?')\n",
    "])\n",
    "country = input(\"ì–´ëŠ ë‚˜ë¼ ìˆ˜ë„ê°€ ê¶ê¸ˆí•˜ì„¸ìš”?\")\n",
    "prompt = chatPrompt_template.invoke({\"country\":country})\n",
    "# print(\"í”„ë¡¬í”„íŠ¸ :\", prompt, type(prompt))\n",
    "result = llm.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b80c62",
   "metadata": {},
   "source": [
    "# 3. ë‹µë³€ í˜•ì‹ ì»¨íŠ¸ë¡¤í•˜ê¸°\n",
    "- llm.invoke()ì˜ ê²°ê³¼ëŠ” AIMessage() -> stringì´ë‚˜ json\n",
    "## 1) ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œ ì´ìš©\n",
    "- StrOutputParserë¥¼ ì‚¬ìš©í•˜ì—¬ LLMì¶œë ¥(AIMessage)ì„ ë‹¨ìˆœ ë¬¸ìì—´ë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e57ed7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# ëª…ì‹œì ì¸ ì§€ì‹œí•˜ìƒì´ í¬í•¨ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}. Retrun the name of the city only.\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ê°’ ì£¼ì…\n",
    "prompt = prompt_template.invoke({\"country\": \"Korea\"})\n",
    "# print(prompt)\n",
    "ai_message = llm.invoke(prompt)\n",
    "# print(ai_message)\n",
    "# ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œë¥¼ ì´ìš©í•˜ì—¬ llmì‘ë‹µ(AIMessageê°ì²´)ì„ ë‹¨ìˆœ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "output_parser = StrOutputParser()\n",
    "result = output_parser.invoke(ai_message)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a7a9fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë³€ìˆ˜ì„¤ì •, system, few shot ì§€ì •\n",
    "chat_prompt_template= ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant with excpertise in South Korea\"),\n",
    "    ('human',\"What is the capital of Italy?\"),\n",
    "    (\"ai\",\"Rome\"),\n",
    "    ('human',\"What is the capital of France?\"),\n",
    "    (\"ai\",\"Paris\"),\n",
    "    ('human',\"What is the capital of {country}? Return the name of the city only\")\n",
    "])\n",
    "output_parser = StrOutputParser()\n",
    "output_parser.invoke(llm.invoke(chat_prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f9529",
   "metadata": {},
   "source": [
    "## 2) Json ì¶œë ¥ íŒŒì„œ ì´ìš©\n",
    "- {'name':'í™ê¸¸ë™','age':22}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f8e232f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capital': 'Seoul', 'population': 51000000, 'language': 'Korean', 'currency': 'South Korean won'} <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "country_detail_prompt = PromptTemplate(\n",
    "    template = \"\"\"Give following information about {country}.\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    Return in JSON format and return the JSON dictionary only\n",
    "    \"\"\", input_variables=['country'])\n",
    "prompt = country_detail_prompt.invoke({\"country\":\"Korea\"})\n",
    "ai_message = llm.invoke(prompt)\n",
    "output_parser= JsonOutputParser()\n",
    "json_result = output_parser.invoke(ai_message)\n",
    "print(json_result, type(json_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d1ce0f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Invalid json output: ```\n{\n    \"capital\": \"Tokyo\",\n    \"population\": 127,111,594,\n    \"language\": \"Japanese\",\n    \"currency\": \"Yen\"\n}\n```\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\output_parsers\\json.py:84\u001b[0m, in \u001b[0;36mJsonOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\utils\\json.py:156\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[1;34m(json_string, parser)\u001b[0m\n\u001b[0;32m    155\u001b[0m     json_str \u001b[38;5;241m=\u001b[39m json_string \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\utils\\json.py:172\u001b[0m, in \u001b[0;36m_parse_json\u001b[1;34m(json_str, parser)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\utils\\json.py:129\u001b[0m, in \u001b[0;36mparse_partial_json\u001b[1;34m(s, strict)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\json\\__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[1;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 3 column 23 (char 48)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountry_detail_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcountry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjapan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\output_parsers\\base.py:200\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    198\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[1;32m--> 200\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    211\u001b[0m         config,\n\u001b[0;32m    212\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\runnables\\base.py:2058\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   2054\u001b[0m     child_config \u001b[38;5;241m=\u001b[39m patch_config(config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child())\n\u001b[0;32m   2055\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m   2056\u001b[0m         output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   2057\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m-> 2058\u001b[0m             context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   2059\u001b[0m                 call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   2060\u001b[0m                 func,\n\u001b[0;32m   2061\u001b[0m                 input_,\n\u001b[0;32m   2062\u001b[0m                 config,\n\u001b[0;32m   2063\u001b[0m                 run_manager,\n\u001b[0;32m   2064\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2065\u001b[0m             ),\n\u001b[0;32m   2066\u001b[0m         )\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2068\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\runnables\\config.py:433\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    432\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\output_parsers\\base.py:201\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    198\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m--> 201\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    205\u001b[0m             config,\n\u001b[0;32m    206\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    207\u001b[0m         )\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    211\u001b[0m         config,\n\u001b[0;32m    212\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\output_parsers\\json.py:87\u001b[0m, in \u001b[0;36mJsonOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     86\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid json output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg, llm_output\u001b[38;5;241m=\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Invalid json output: ```\n{\n    \"capital\": \"Tokyo\",\n    \"population\": 127,111,594,\n    \"language\": \"Japanese\",\n    \"currency\": \"Yen\"\n}\n```\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "output_parser.invoke(llm.invoke(country_detail_prompt.invoke({\"country\":\"japan\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b997e",
   "metadata": {},
   "source": [
    "## 3) êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‚¬ìš©\n",
    "- Pydantic ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ LLMì¶œë ¥ì„ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ë°›ê¸°(JsonParserë³´ë‹¤ í›¨ì”¬ ì•ˆì •ì )\n",
    "- Pydantic : ë°ì´í„°ìœ íš¨ì„±ê²€ì‚¬, ì„¤ì •ê´€ë¦¬ë¥¼ ê°„í¸í•˜ê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7dd56901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "    def __init__(self, id, name, is_active=True):\n",
    "        self.id   = id\n",
    "        self.name = name\n",
    "        self.is_active = is_active\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "user = User(1, \"í™ê¸¸ë™\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3590a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1 name='í™ê¸¸ë™' is_active=True\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class User(BaseModel):\n",
    "    id:int = Field(gt=0, description=\"id\")\n",
    "    name:str = Field(min_length=2, description=\"name\")\n",
    "    is_active:bool = Field(default=True, description=\"idí™œì„±í™” ì—¬ë¶€\")\n",
    "user = User(id=1, name=\"í™ê¸¸ë™\")\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da013f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountryDetail(capital='Seoul', population=51, language='Korean', currency='South Korean won')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_detail_prompt = PromptTemplate(\n",
    "    template = \"\"\"Give following information about {country}.\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    Return in JSON format and return the JSON dictionary only\"\"\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "class CountryDetail(BaseModel): #description: ë” ì •í™•í•œ ì¶œë ¥ ìœ ë„\n",
    "    capital:str  = Field(description=\"the capital of the country\")\n",
    "    population:int = Field(description=\"the population of the country\")\n",
    "    language:str = Field(description=\"the language of the country\")\n",
    "    currency:str = Field(description=\"the currency of the country\")\n",
    "# ì¶œë ¥ í˜•ì‹ íŒŒì„œ + LLM\n",
    "structedllm = llm.with_structured_output(CountryDetail)\n",
    "# llm.invoke(country_detail_prompt.invoke({\"country\":\"Korea\"}))\n",
    "info = structedllm.invoke(country_detail_prompt.invoke({\"country\":\"Korea\"}))\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50f230b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CountryDetail"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c325b9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Seoul', 51, 'Korean', 'South Korean won')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.capital, info.population, info.language, info.currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3cd3903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infoë¥¼ json ìŠ¤íƒ€ì¼ë¡œ : {\"capital\":\"Seoul\",\"population\":51,\"language\":\"Korean\",\"currency\":\"South Korean won\"}\n",
      "infoë¥¼ dictë¡œ : {'capital': 'Seoul', 'population': 51, 'language': 'Korean', 'currency': 'South Korean won'}\n",
      "infoë¥¼ dictë¡œ : {'capital': 'Seoul', 'population': 51, 'language': 'Korean', 'currency': 'South Korean won'}\n"
     ]
    }
   ],
   "source": [
    "print(\"infoë¥¼ json ìŠ¤íƒ€ì¼ë¡œ :\", info.model_dump_json())\n",
    "print(\"infoë¥¼ dictë¡œ :\", info.model_dump())\n",
    "print(\"infoë¥¼ dictë¡œ :\", info.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8da20150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(info.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8bc158",
   "metadata": {},
   "source": [
    "# 4. LCELì„ í™œìš©í•œ ë ì²´ì¸ ìƒì„±í•˜ê¸°\n",
    "## 1) ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œ ì‚¬ìš©\n",
    "- invoke\n",
    "- StrOutputParser, ChatOllama, PromptTemplateë“±ì€ ëª¨ë‘ Runnableë¡œë¶€í„° ìƒì† ë°›ìŒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1fcc1b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\",\n",
    "                temperature=0)\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}. Retrun the name of the city only.\",\n",
    "    input_variables = [\"country\"])\n",
    "output_parser = StrOutputParser()\n",
    "output_parser.invoke(llm.invoke(chat_prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a109b5c6",
   "metadata": {},
   "source": [
    "## 2) LCELì„ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ì²´ì¸ êµ¬ì„±\n",
    "- íŒŒì´í”„ì—°ì‚°ì(|) ì´ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db1a2d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ -> llm -> ì¶œë ¥íŒŒì„œë¥¼ ì—°ê²°í•˜ëŠ” ì²´ì¸ ìƒì„±\n",
    "capital_chain = prompt_template | llm | output_parser\n",
    "# ìƒì„±ëœ ì²´ì¸ invoke\n",
    "capital_chain.invoke({\"country\":\"Korea\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15c7ec",
   "metadata": {},
   "source": [
    "## 3) ë³µí•©ì²´ì¸ êµ¬ì„±\n",
    "- ì—¬ëŸ¬ ë‹¨ê³„ì˜ ì¶”ë¡ ì´ í•„ìš”í•œ ê²½ìš°(ì²´ì¸ ì—°ê²°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f655d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‚˜ë¼ ì„¤ëª… -> ë‚˜ë¼ì´ë¦„\n",
    "country_prompt = PromptTemplate(\n",
    "    template=\"\"\"Guess the name of the country based on the following informat:\n",
    "    {information}\n",
    "    Return the name of the country only\"\"\",\n",
    "    input_variables=[\"information\"]\n",
    ")\n",
    "output_parser.invoke(llm.invoke(country_prompt.invoke({\"information\":\n",
    "                            \"This country is very famous for its wine\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7334c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‚˜ë¼ëª… ì¶”ì¶œ ì²´ì¸ ìƒì„±\n",
    "country_chain = country_prompt | llm | output_parser\n",
    "country_chain.invoke({\"information\":\"This country is very famous for its wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee39489e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë³µí•©ì²´ì¸ : ë‚˜ë¼ì„¤ëª… -> ë‚˜ë¼ëª…(country_chain)\n",
    "#                     ë‚˜ë¼ëª… -> ìˆ˜ë„(capital_chain)\n",
    "final_chain = country_chain | capital_chain\n",
    "final_chain.invoke({\"information\":\"This country is very famous for its wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "817c0e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³µí•©ì²´ì¸ : information -> country_chain -> (ë‚˜ë¼ëª…ì„ country) -> capital_chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "final_chain = {\"information\":RunnablePassthrough()} | \\\n",
    "                {\"country\":country_chain} | capital_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6cce37b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain.invoke({\"information\":\"This country is very famous for its wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38794d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm(ipykernel)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "182.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
